{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedGroupKFold,StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.impute import KNNImputer # IterativeImputer,\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, balanced_accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_read_file(file_name):\n",
    "    ''' Read location csv / xlsx   '''\n",
    "    print('=== Starting Reading === ')\n",
    "\n",
    "    if file_name.endswith(\"csv\"):\n",
    "        df = pd.read_csv(file_name, header=0)\n",
    "    else:\n",
    "        df = pd.read_excel(file_name, header=0)\n",
    "    \n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    print(df.columns, '\\n')\n",
    "    print(f\"The input data shape: {df.shape[0]}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# One-hot encode categorical features\n",
    "def preprocess_data(df):\n",
    "    df = pd.get_dummies(df, columns=['placenta_cat', 'baby_gender'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_filename(base_filename, folder='./evaluation_result/'):\n",
    "    files = os.listdir(folder)\n",
    "    \n",
    "    matching_files = [f for f in files if f.startswith(base_filename)]\n",
    "    \n",
    "    if matching_files:\n",
    "        numbers = [int(f.split('_')[-1].split('.')[0]) for f in matching_files if '_' in f]\n",
    "        next_number = max(numbers) + 1\n",
    "    else:\n",
    "        next_number = 1\n",
    "    \n",
    "    next_filename = f\"{base_filename}_{next_number}.csv\"\n",
    "    return os.path.join(folder, next_filename)\n",
    "\n",
    "\n",
    "def fit_and_evaluate(X_train, X_test, y_train, y_test, methods, model_name='lr',  csv_filename='./evaluation_result/model_evaluation_results.csv'):\n",
    "    ''' modeling and evaluation, 'model_name' is the notes inside of saving csv'''\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "\n",
    "    precision = round(precision_score(y_test, y_pred), 4)\n",
    "    recall = round(recall_score(y_test, y_pred), 4)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "    f1 = round(f1_score(y_test, y_pred), 4)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred), 4)\n",
    "    roc_auc = round(roc_auc_score(y_test, y_proba), 4)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = round(tn / (tn + fp), 4)\n",
    "    npv = round(tn / (tn + fn), 4)\n",
    "    \n",
    "    # Compute true positive rate (sensitivity) for 10% FPR\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "\n",
    "    results = {\n",
    "        'Method' : methods,\n",
    "        'Model': model_name,\n",
    "\n",
    "        'Precision': precision,\n",
    "        'Negative Predictive Value': npv,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "\n",
    "        'Balanced Accuracy': bal_acc,\n",
    "        'ROC AUC': roc_auc,\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"'Method': {methods}, Model: {model_name}\")\n",
    "    for key, value in results.items():\n",
    "        if (key != 'Model') & (key == 'ROC AUC'):\n",
    "            print(f\"{key}: {value}\")\n",
    "    # print(\"=\"*50)\n",
    "\n",
    "        \n",
    "\n",
    "    # Write to CSV\n",
    "    file_exists = False\n",
    "    directory = './evaluation_result/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    try:\n",
    "        with open(csv_filename, 'r') as csvfile:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        file_exists = False\n",
    "\n",
    "    with open(csv_filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=results.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(results)\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    'lr': LogisticRegression(class_weight='balanced', random_state=123), # , max_iter=300\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'svcrbf': SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=123), # rbf\n",
    "    'rf': RandomForestClassifier(class_weight='balanced', random_state=123),\n",
    "    'xgb': XGBClassifier(random_state=123), #scale_pos_weight=2.26  1\n",
    "    'lgbm': LGBMClassifier(class_weight='balanced', random_state=123, verbose=0),\n",
    "    'catboost': CatBoostClassifier(class_weights=[1, 1], random_state=123, verbose=0),\n",
    "    'mlp': MLPClassifier(random_state=123)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"./preprocessing/df_tri2_12w28w_drop_death_train_bw.csv\"\n",
    "df_read = f_read_file(file_name)\n",
    "print()\n",
    "df_read.count_id.unique().shape  \n",
    "\n",
    "# remove illegitimate features\n",
    "df_read = df_read.drop(columns=['birth_ga', 'birth_weight']) \n",
    "df_read.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df)\n",
    "\n",
    "df.shape, df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L1.1a] Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance handling\n",
    "def handle_imbalance(X, y, method='ROS'):\n",
    "    print(f'{method} is using.')\n",
    "    if method == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=123)\n",
    "    elif method == 'ROS':\n",
    "        sampler = RandomOverSampler(random_state=123)\n",
    "    elif method == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=123)\n",
    "    elif method == 'RUS':\n",
    "        sampler = RandomUnderSampler(random_state=123)\n",
    "    else:\n",
    "        print('********** There is not methods to handle imbalance in Sampling Spliting')\n",
    "        return X, y\n",
    "        raise ValueError(\"Unknown method: \" + method)\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "\n",
    "# Split data and apply imbalance handling after split\n",
    "def imbalance_after_split(df, method):\n",
    "    X = df.drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id'])\n",
    "    y = df['WHO_sga']\n",
    "    print(X.columns)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\n",
    "    train_index, test_index = next(sss.split(X, y))\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    X_train_res, y_train_res = handle_imbalance(X_train, y_train, method)  # Change method as needed\n",
    "    \n",
    "    return X_train_res, X_test, y_train_res, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Split data and apply imbalance handling before split\n",
    "def imbalance_before_split(df, method):\n",
    "    X = df.drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id'])\n",
    "    y = df['WHO_sga']\n",
    "    print(X.columns)\n",
    "    \n",
    "    X_res, y_res = handle_imbalance(X, y, method)  # Change method as needed\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\n",
    "    train_index, test_index = next(sss.split(X_res, y_res))\n",
    "    \n",
    "    X_train, X_test = X_res.iloc[train_index], X_res.iloc[test_index]\n",
    "    y_train, y_test = y_res.iloc[train_index], y_res.iloc[test_index]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df)\n",
    "\n",
    "df = df.drop_duplicates(subset='count_id', keep='first')\n",
    "df.shape, df.WHO_sga.value_counts()\n",
    "\n",
    "# ((4749, 17),\n",
    "# WHO_sga\n",
    "#  0    3295\n",
    "#  1    1454"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "# Split data and apply imbalance handling after split\n",
    "methods = ['ROS', 'SMOTE', 'ADASYN', 'RUS', 'None']\n",
    "\n",
    "for method in methods:\n",
    "    X_train, X_test, y_train, y_test = imbalance_after_split(df, method)  # imbalance_after_split(df)\n",
    "    X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "    # Evaluate all models\n",
    "    for model_name in models.keys():\n",
    "        fit_and_evaluate(X_train, X_test, y_train, y_test, methods=method, model_name=model_name, csv_filename='./evaluation_result/L1.1a_proper.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect\n",
    "# Split data and apply imbalance handling before split\n",
    "methods = ['ROS', 'SMOTE', 'ADASYN', 'RUS'] # , 'None'\n",
    "\n",
    "for method in methods:\n",
    "    X_train, X_test, y_train, y_test = imbalance_before_split(df, method)  # imbalance_after_split(df)\n",
    "    X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "    # Evaluate all models\n",
    "    for model_name in models.keys():\n",
    "        fit_and_evaluate(X_train, X_test, y_train, y_test, methods=method, model_name=model_name, csv_filename='./evaluation_result/L1.1a_improper.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('model_evaluation_results.csv')\n",
    "# results_df[-21:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L1.1b] Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data\n",
    "def split_data_sgkf(df):\n",
    "    X = df.drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id'])\n",
    "    y = df['WHO_sga']\n",
    "    # print(X.columns)\n",
    "    \n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups=df.count_id))\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def remove_features(X, N):\n",
    "    if N != 0:\n",
    "        np.random.seed(123)\n",
    "        # X = pd.DataFrame(X)\n",
    "        cols = list(X.columns)\n",
    "        for index in range(X.shape[0]):\n",
    "            cols_to_nan = np.random.choice(cols, N, replace=False)\n",
    "            X.loc[index, cols_to_nan] = np.nan\n",
    "        X = X.dropna(how='all')\n",
    "    return X\n",
    "\n",
    "\n",
    "# Function to impute missing values\n",
    "def impute_missing_values(method, X_train):\n",
    "    if method == 'mean':\n",
    "        ga_col = 'ga'\n",
    "        # if ga_col not in X.columns:\n",
    "        #     raise ValueError(f\"Column '{ga_col}' not found in dataframe\")\n",
    "        for col in X_train.columns:\n",
    "            if col != ga_col:\n",
    "                means = X_train.groupby(ga_col)[col].transform('mean')\n",
    "                X_train = X_train.fillna(means)\n",
    "            else:\n",
    "                means = X_train.mean()\n",
    "                X_train = X_train.fillna(means)\n",
    "        return X_train.values\n",
    "    elif method == 'iterative': # MICE\n",
    "        imputer = IterativeImputer(random_state=123)\n",
    "    elif method == 'knn':\n",
    "        imputer = KNNImputer()\n",
    "    elif method == 'rf':\n",
    "        imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=123), \n",
    "                               missing_values=np.nan, max_iter=10, random_state=123)\n",
    "    elif method == 'xgb':\n",
    "        imputer = IterativeImputer(estimator=XGBRegressor(n_estimators=10, random_state=123, verbosity=0), \n",
    "                               missing_values=np.nan, max_iter=10, random_state=123)\n",
    "    elif method == 'lgbm':\n",
    "        imputer = IterativeImputer(estimator=LGBMRegressor(n_estimators=10, random_state=123, verbosity=0),\n",
    "                               missing_values=np.nan, max_iter=10, random_state=123)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown imputation method: \" + method)\n",
    "    \n",
    "    print(f'===== {method} is using =====')\n",
    "    X_imputed  = imputer.fit_transform(X_train)\n",
    "    return X_imputed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df)\n",
    "df.columns\n",
    "\n",
    "con_cols = ['ga', 'efw', 'bpd', 'hc', 'ac', 'fl', 'nf', 'cm', 'tcd', 'm_age']\n",
    "cat_cols = ['placenta_cat_Low_Lying', 'placenta_cat_Upper_Segment', 'baby_gender_BOY', 'baby_gender_GIRL']\n",
    "\n",
    "df.shape, df.WHO_sga.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## proper\n",
    "## runs: 83m 31.7s\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "for N in range(7): \n",
    "    \n",
    "    print(f\"======= Removing {N} features ========\")\n",
    "    X_train_removed = remove_features(X_train[con_cols], N)\n",
    "    X_train_removed = pd.concat([X_train_removed, X_train[cat_cols]], axis=1)\n",
    "    # X_train_removed_df = pd.DataFrame(X_train_removed)\n",
    "    # X_train_removed_df.to_csv(f'X_train_removed_{N}.csv', index=False)\n",
    "\n",
    "    for model_name in models.keys():\n",
    "\n",
    "        for method in ['iterative', 'mean', 'knn', 'rf', 'xgb', 'lgbm']: #  'iterative', 'mean', 'knn', 'rf', 'xgb', 'lgbm'\n",
    "            if X_train_removed.notna().all(axis=None):\n",
    "                (f\"--- Missing value {N} ----\")\n",
    "                fit_and_evaluate(X_train_removed, X_test, y_train, y_test, methods='no missing value', model_name=model_name, csv_filename='./evaluation_result/L1.1b_proper.csv')\n",
    "                break\n",
    "            else:\n",
    "                print(f\"--- Missing value {N}. Imputing with {method} method---\")\n",
    "                X_train_imputed = impute_missing_values(method, X_train=X_train_removed)\n",
    "                fit_and_evaluate(X_train_imputed, X_test, y_train, y_test, methods=f'{N}_'+method, model_name=model_name, csv_filename='./evaluation_result/L1.1b_proper.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improper\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "\n",
    "# Remove features and impute\n",
    "times_ = 7\n",
    "for N in range(times_): \n",
    "\n",
    "    print(f\"======= Removing {N} features ========\")\n",
    "    X_train_removed = remove_features(X_train, N)\n",
    "\n",
    "    X_train_removed_with_test = pd.concat([X_train_removed, X_test], ignore_index=True)\n",
    "    # X_train_removed_df = pd.DataFrame(X_train_removed)\n",
    "    # X_train_removed_df.to_csv(f'X_train_removed_{N}.csv', index=False)\n",
    "\n",
    "    for model_name in models.keys():\n",
    "\n",
    "        for method in ['iterative', 'mean', 'knn', 'rf', 'xgb', 'lgbm']: # \n",
    "            if X_train_removed.notna().all(axis=None):\n",
    "                (f\"--- Missing value {N} ----\")\n",
    "                fit_and_evaluate(X_train_removed, X_test, y_train, y_test, methods='no missing value', model_name=model_name, csv_filename='./evaluation_result/L1.1b_improper.csv')\n",
    "                break\n",
    "            else:\n",
    "                print(f\"--- Missing value {N}. Imputing with {method} method---\")\n",
    "                X_train_imputed_wrong = impute_missing_values(X_train_removed_with_test, method, X_train=X_train_removed_with_test)\n",
    "\n",
    "                X_train_imputed_train_wrong = X_train_imputed_wrong[:len(X_train_removed)]#.iloc[:-len(X_test)]\n",
    "                # X_train_imputed_train_wrong = pd.DataFrame(X_train_imputed_train_wrong)\n",
    "                # X_train_imputed_train_wrong.to_csv(f'111.csv', index=False)\n",
    "                \n",
    "                fit_and_evaluate(X_train_imputed_train_wrong, X_test, y_train, y_test, methods=f'{N}_'+method, model_name=model_name, csv_filename='./evaluation_result/L1.1b_improper.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L1.1c] Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_params = {}\n",
    "\n",
    "# Function to standardize the 'training data'\n",
    "def df_standardize(df, cols, Standardize):\n",
    "    continuous_col = df.columns[df.columns.isin(cols)]\n",
    "    for col in continuous_col:\n",
    "        try:\n",
    "            df[col], params = scale_feature(df[col], method=Standardize)\n",
    "            scaler_params[col] = params\n",
    "        except Exception as e:\n",
    "            print(f\"Train set: Error in column {col}: {e}\")\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to apply saved standardization parameters to the 'test data'\n",
    "def df_standardize_test(df, cols, Standardize):\n",
    "    continuous_col = df.columns[df.columns.isin(cols)]\n",
    "    for col in continuous_col:\n",
    "        try:\n",
    "            if col in scaler_params:\n",
    "                df[col] = scale_feature_test(df[col], scaler_params[col], method=Standardize)\n",
    "            else:\n",
    "                print(f\"Test set: Parameters for column {col} not found in scaler_params.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in column {col}: {e}\")\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to scale features in the 'training set'\n",
    "def scale_feature(col, method='zscore'):\n",
    "    params = {}\n",
    "    if method == 'zscore':\n",
    "        std = np.std(col)\n",
    "        mean = np.mean(col)\n",
    "        params = {'mean': mean, 'std': std}\n",
    "        return (col - mean) / std, params\n",
    "    elif method == 'minmax':\n",
    "        min_val = np.min(col)\n",
    "        max_val = np.max(col)\n",
    "        params = {'min': min_val, 'max': max_val}\n",
    "        return (col - min_val) / (max_val - min_val), params\n",
    "    elif method == 'mean_normal':\n",
    "        mean = np.mean(col)\n",
    "        range_val = np.max(col) - np.min(col)\n",
    "        params = {'mean': mean, 'range_val': range_val}\n",
    "        return (col - mean) / range_val, params\n",
    "    elif method == 'power':\n",
    "        transformer = PowerTransformer(method='yeo-johnson')\n",
    "        transformed_col = transformer.fit_transform(col.values.reshape(-1, 1)).flatten()\n",
    "        params = {'transformer': transformer}\n",
    "        return transformed_col, params\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"**** Invalid normalization method. Supported methods are 'zscore', 'minmax', and 'mean_normal'\")\n",
    "\n",
    "\n",
    "# Function to scale features in the 'test set' using saved parameters\n",
    "def scale_feature_test(col, params, method='zscore'):\n",
    "    if method == 'zscore':\n",
    "        mean = params['mean']\n",
    "        std = params['std']\n",
    "        return (col - mean) / std\n",
    "    elif method == 'minmax':\n",
    "        min_val = params['min']\n",
    "        max_val = params['max']\n",
    "        return (col - min_val) / (max_val - min_val)\n",
    "    elif method == 'mean_normal':\n",
    "        mean = params['mean']\n",
    "        range_val = params['range_val']\n",
    "        return (col - mean) / range_val\n",
    "    elif method == 'power':\n",
    "        transformer = params['transformer']\n",
    "        transformed_col = transformer.transform(col.values.reshape(-1, 1)).flatten()\n",
    "        return transformed_col\n",
    "    else:\n",
    "        raise ValueError(\"**** Invalid normalization method. Supported methods are 'zscore', 'minmax', and 'mean_normal'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "\n",
    "cols_to_standardize = ['ga', 'efw', 'bpd', 'hc', 'ac', 'fl', 'nf', 'cm', 'tcd', 'm_age']\n",
    "scalers = ['zscore', 'minmax', 'mean_normal', 'power', ] # \n",
    "\n",
    "for scaler in scalers:\n",
    "\n",
    "    for model_name in models.keys():\n",
    "\n",
    "        X_train_standardized = df_standardize(X_train.copy(), cols_to_standardize, scaler)\n",
    "        X_test_standardized = df_standardize_test(X_test.copy(), cols_to_standardize, scaler)\n",
    "        # print(X_train_standardized.shape, X_test_standardized.shape)    \n",
    "\n",
    "        fit_and_evaluate(X_train_standardized, X_test_standardized, y_train, y_test, methods=scaler, model_name=model_name, csv_filename='./evaluation_result/L1.1c_proper.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing for 'mean_normal'\n",
    "X_train_standardized = df_standardize(X_train.copy(), cols_to_standardize, 'mean_normal')\n",
    "scaler_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "# df = df.drop(columns=['WHO_cur_sga', 'count_id'])\n",
    "\n",
    "cols_to_standardize = ['ga', 'efw', 'bpd', 'hc', 'ac', 'fl', 'nf', 'cm', 'tcd', 'm_age']\n",
    "scalers = ['zscore', 'minmax', 'mean_normal', 'power', ] # 'zscore', 'power', 'minmax', 'mean_normal'\n",
    "\n",
    "for scaler in scalers:\n",
    "\n",
    "    df_standardized = df_standardize(df.copy(), cols_to_standardize, scaler)\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        X_train_standardized, X_test_standardized, y_train, y_test = split_data_sgkf(df_standardized)\n",
    "        # print(X_train_standardized.shape, X_test_standardized.shape)    \n",
    "        # print(X_train_standardized.head)\n",
    "        fit_and_evaluate(X_train_standardized, X_test_standardized, y_train, y_test, methods=scaler, model_name=model_name, csv_filename='./evaluation_result/L1.1c_improper.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing for 'mean_normal'\n",
    "df_standardized = df_standardize(df.copy(), cols_to_standardize, 'mean_normal')\n",
    "scaler_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L1.2] Feature selection on training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_lr(X, y):\n",
    "    model = LogisticRegression(class_weight='balanced', random_state=123) # , max_iter=1000\n",
    "    model.fit(X, y)\n",
    "    feature_scores = model.coef_[0]\n",
    "    return feature_scores\n",
    "\n",
    "def feature_selection_rf(X, y):\n",
    "    model = RandomForestClassifier(class_weight='balanced', random_state=123)\n",
    "    model.fit(X, y)\n",
    "    feature_scores = model.feature_importances_\n",
    "    return feature_scores\n",
    "\n",
    "def feature_selection_xgboost(X, y):\n",
    "    model = XGBClassifier(random_state=123)\n",
    "    model.fit(X, y)\n",
    "    feature_scores = model.feature_importances_\n",
    "    return feature_scores\n",
    "\n",
    "def feature_selection_lgbm(X, y):\n",
    "    model = LGBMClassifier(class_weight='balanced', random_state=123, verbose=0)\n",
    "    model.fit(X, y)\n",
    "    feature_scores = model.feature_importances_\n",
    "    return feature_scores\n",
    "\n",
    "def feature_selection_catboost(X, y):\n",
    "    model = CatBoostClassifier(random_state=123, verbose=0)\n",
    "    model.fit(X, y)\n",
    "    feature_scores = model.get_feature_importance()\n",
    "    return feature_scores\n",
    "\n",
    "\n",
    "\n",
    "def save_feature_scores_to_csv(feature_scores, method, dataset, feature_names, csv_filename='./evaluation_result/L1.2_feature importance.csv'):\n",
    "    scores_df = pd.DataFrame([feature_scores], columns=feature_names)\n",
    "    scores_df['Method'] = method\n",
    "    scores_df['Dataset'] = dataset\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(csv_filename, 'r') as csvfile:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        file_exists = False\n",
    "\n",
    "    with open(csv_filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Method', 'Dataset'] + feature_names)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(scores_df.iloc[0].to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "\n",
    "X_entire, y_entire = df.copy().drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id']),  df.WHO_sga \n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "\n",
    "feature_names = X_entire.columns.tolist()\n",
    "feature_selections = {\n",
    "    'lr': feature_selection_lr,\n",
    "    'rf': feature_selection_rf,\n",
    "    'lgbm': feature_selection_lgbm,\n",
    "    'catboost': feature_selection_catboost,\n",
    "    'xgb' : feature_selection_xgboost\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "        'entire': [X_entire, y_entire],\n",
    "        'train': [X_train, y_train],\n",
    "        'test': [X_test, y_test]\n",
    "}\n",
    "\n",
    "for method_name, feature_selection_func in feature_selections.items():\n",
    "        for dataset_name, data in datasets.items():\n",
    "                # Perform feature selection on the entire dataset (incorrect)\n",
    "                feature_scores = feature_selection_func(data[0], data[1])\n",
    "                save_feature_scores_to_csv(feature_scores, method_name, dataset_name, feature_names)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on file 'results_df', we will selection features from the top N features (N = 0, 1,..., 7), details on section: auto-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('./evaluation_result/L1.2_feature importance.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(scores, top_n=5):\n",
    "    # Calculate the absolute values of scores and maintain their associated feature names\n",
    "    scores_abs = scores.abs()\n",
    "    sorted_features = scores_abs.sort_values(ascending=False)\n",
    "    top_features = sorted_features.head(top_n)\n",
    "\n",
    "    return top_features.index.tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preformance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on 'results_df': feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# n = len(feature_names)\n",
    "\n",
    "## select the classifier\n",
    "## accroding to the 'results_df', each classifier is in diff line: 0, 3, 6, 9, 12\n",
    "indexx_n = [0, 3, 6, 9, 12] # 0, 3, 6, 9, 12\n",
    "feature_names = ['ga', 'efw', 'bpd', 'hc', 'ac', 'fl', 'nf', 'cm', 'tcd', 'm_age', 'placenta_cat_Low_Lying', 'placenta_cat_Upper_Segment', 'baby_gender_BOY', 'baby_gender_GIRL']\n",
    "\n",
    "for i in indexx_n:\n",
    "    classifier_entire = results_df.loc[i, feature_names]\n",
    "    classifier_train = results_df.loc[i+1, feature_names]\n",
    "    classifier_test = results_df.loc[i+2, feature_names]\n",
    "\n",
    "    for n in range(7): ## select top 7 factors\n",
    "        n+=1\n",
    "        \n",
    "        top_features_entire = select_top_features(classifier_entire, top_n=n)\n",
    "        print(f\"Top {n} features ({results_df.Method[i]} in {results_df.Dataset[i]}):\", top_features_entire)\n",
    "\n",
    "        top_features_train = select_top_features(classifier_train, top_n=n)\n",
    "        print(f\"Top {n} features ({results_df.Method[i+1]} in {results_df.Dataset[i+1]}) :\", top_features_train)\n",
    "\n",
    "        # top_features_test = select_top_features(classifier_test, top_n=n)\n",
    "        # print(f\"Top {n} features ({results_df.Method[i+2]} in {results_df.Dataset[i+2]}) :\", top_features_test)\n",
    "\n",
    "\n",
    "        selected_features_ = {\n",
    "            'entire': top_features_entire,\n",
    "            'train': top_features_train,\n",
    "            # 'test': top_features_test\n",
    "        }\n",
    "\n",
    "        for fea_sel_name, selected_features_train in selected_features_.items():\n",
    "\n",
    "            fit_and_evaluate(X_train[selected_features_train], X_test[selected_features_train], y_train, y_test, methods=str(n)+'_'+fea_sel_name, model_name=results_df.Method[i+2], csv_filename='./evaluation_result/L1.2_result.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('./evaluation_result/L1.2_result.csv', header=0)\n",
    "\n",
    "# Extract the group number from the Method column and create separate columns for train and entire sets\n",
    "df4['Group'] = df4['Method'].str.extract(r'(\\d+)', expand=False)\n",
    "df4['Set'] = df4['Method'].apply(lambda x: 'test' if 'test' in x else ('train' if 'train' in x else 'entire'))\n",
    "# Select the required columns and pivot the dataframe\n",
    "df_pivot = df4.pivot_table(index=['Group', 'Model'], columns='Set', values='ROC AUC').reset_index()\n",
    "\n",
    "# Define new colors and markers for different models\n",
    "models_reordered = df_pivot['Model'].unique()\n",
    "colors_reordered = ['green', 'purple', 'black', 'grey', 'blue', 'yellow', 'orange',  'brown']\n",
    "marker = 'o'\n",
    "legend_labels = [model.upper() for model in models_reordered]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Define unique groups for grouping\n",
    "groups = df_pivot['Group'].unique()\n",
    "\n",
    "# Define the x positions for each group with some offset for train and entire sets\n",
    "x_positions = {group: [i * 1.3, i * 1.3 + 0.6] for i, group in enumerate(groups)}\n",
    "\n",
    "# Plot data for each model\n",
    "for j, model in enumerate(models_reordered):\n",
    "    for group in groups:\n",
    "        feature_data_group = df_pivot[(df_pivot['Model'] == model) & (df_pivot['Group'] == group)]\n",
    "        roc_auc_train = feature_data_group['train'].values\n",
    "        roc_auc_entire = feature_data_group['entire'].values\n",
    "        \n",
    "        # Offset for minor separation within the group\n",
    "        offset = j * 0.05\n",
    "        \n",
    "        # Plot lines to connect entire and train\n",
    "        if len(roc_auc_train) > 0 and len(roc_auc_entire) > 0:\n",
    "            ax.plot([x_positions[group][0] + offset, x_positions[group][1] + offset], [roc_auc_train[0], roc_auc_entire[0]], linestyle='--', color='lightgrey', zorder=1)\n",
    "        \n",
    "        # Plot points with larger size and black edge\n",
    "        if len(roc_auc_train) > 0:\n",
    "            ax.scatter(x_positions[group][0] + offset, roc_auc_train[0], marker=marker, color=colors_reordered[j], edgecolor='black', s=100, zorder=2)\n",
    "        if len(roc_auc_entire) > 0:\n",
    "            ax.scatter(x_positions[group][1] + offset, roc_auc_entire[0], marker=marker, color=colors_reordered[j], edgecolor='black', s=100, zorder=2)\n",
    "\n",
    "# Set labels and title\n",
    "# ax.set_xlabel('Groups', fontsize=14)\n",
    "ax.set_ylabel('AUROC', fontsize=18)\n",
    "\n",
    "ax.set_title('AUROC for Dfferent Models after Features Selection', fontsize=23, fontdict={'fontname': 'Times New Roman', 'fontweight': 'bold'},pad=20)\n",
    "\n",
    "# Set custom x-ticks and labels\n",
    "xtick_positions = [pos for positions in x_positions.values() for pos in positions]\n",
    "xtick_labels = [label for group in groups for label in [f'Training', f'Entire']]\n",
    "feature_labels = [f'{group} Features' for group in groups]\n",
    "\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_xticklabels(xtick_labels, fontsize=14) # , rotation=45, ha='right'\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "    ax.text(i * 1.3 + 0.3, ax.get_ylim()[0] - 0.57, feature_labels[i], ha='center', va='top', fontsize=18, color='black', transform=ax.get_xaxis_transform())\n",
    "\n",
    "# Create custom legend with uppercase labels\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor=colors_reordered[i], markeredgecolor='black', markersize=10, label=legend_labels[i])\n",
    "    for i in range(len(models_reordered))\n",
    "]\n",
    "# ax.legend(handles=legend_elements, loc='lower right', bbox_to_anchor=(1, 0.5), fontsize=13) # , title='Models'\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=13) # , title='Models'\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L1.3] Duplicates in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, 101, 10):  # Iterate over percentages from 0% to 100%\n",
    "    for model_name in models.keys():\n",
    "\n",
    "        if n > 0:\n",
    "            sample_size = int(len(X_test) * (n / 100))\n",
    "            X_test_sample = X_test.sample(n=sample_size, replace=False, random_state=123)\n",
    "            y_test_sample = y_test[X_test_sample.index]\n",
    "            \n",
    "            X_train_merged = pd.concat([X_train, X_test_sample], ignore_index=True)\n",
    "            y_train_merged = np.concatenate([y_train, y_test_sample])\n",
    "        else:\n",
    "            X_train_merged = X_train\n",
    "            y_train_merged = y_train\n",
    "\n",
    "        fit_and_evaluate(X_train_merged, X_test, y_train_merged, y_test, methods=str(n), model_name=model_name, csv_filename='./evaluation_result/L1.3_duplicates.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L2] Illegitimate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_bw = f\"./preprocessing/df_tri2_12w28w_drop_death_train_bw.csv\"\n",
    "df_read_bw = f_read_file(file_name_bw)\n",
    "print()\n",
    "df_read_bw.count_id.unique().shape  # 4749\n",
    "\n",
    "\n",
    "''' Select the coloums to be deleted'''\n",
    "df_read_bw.drop(columns=['birth_weight', 'birth_ga'], inplace=True)\n",
    "# df_read_bw.drop(columns=['birth_weight'], inplace=True)\n",
    "# df_read_bw.drop(columns=['birth_ga'], inplace=True)\n",
    "\n",
    "# df_read_bw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read_bw.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "X_train, X_test, y_train, y_test = split_data_sgkf(df)\n",
    "\n",
    "cols_to_standardize = ['ga', 'efw', 'bpd', 'hc', 'ac', 'fl', 'nf', 'cm', 'tcd', 'm_age'] # 'birth_ga', 'birth_weight', \n",
    "illegal_feature = 'birth_weight&ga' # None, birth_ga, birth_weight, birth_weight&ga\n",
    "\n",
    "for model_name in models.keys():\n",
    "\n",
    "    X_train_standardized = df_standardize(X_train.copy(), cols_to_standardize, 'zscore')\n",
    "    X_test_standardized = df_standardize_test(X_test.copy(), cols_to_standardize, 'zscore')\n",
    "        \n",
    "    fit_and_evaluate(X_train_standardized, X_test_standardized, y_train, y_test, methods=illegal_feature, model_name=model_name, csv_filename='./evaluation_result/L2_illegimate.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L3] Non-independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_sgkf(df):\n",
    "    X = df.drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id'])\n",
    "    y = df['WHO_sga']    \n",
    "    count_id = df.count_id\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups=count_id))\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    count_id_train, count_id_test = count_id.iloc[train_index], count_id.iloc[test_index]\n",
    "    common_count_ids = set(count_id_train).intersection(set(count_id_test))\n",
    "    print(f\"sgkf - Number of common 'count_id' values between train and test sets: {len(common_count_ids)}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def split_data_skf(df):\n",
    "    X = df.drop(columns=['WHO_sga', 'WHO_cur_sga', 'count_id'])\n",
    "    y = df['WHO_sga']\n",
    "    count_id = df.count_id\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    train_index, test_index = next(skf.split(X, y))\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    count_id_train, count_id_test = count_id.iloc[train_index], count_id.iloc[test_index]\n",
    "    common_count_ids = set(count_id_train).intersection(set(count_id_test))\n",
    "    print(f\"skf -Number of common 'count_id' values between train and test sets: {len(common_count_ids)}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read.copy()\n",
    "df = preprocess_data(df) # onehot\n",
    "\n",
    "split_methods = {\n",
    "    'sgkf' : split_data_sgkf,\n",
    "    'skf' : split_data_skf\n",
    "}\n",
    "\n",
    "for split_methond_name, split_methond in split_methods.items():\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_methond(df)\n",
    "\n",
    "    \n",
    "    for model_name in models: # .keys()\n",
    "        \n",
    "        fit_and_evaluate(X_train, X_test, y_train, y_test, methods=split_methond_name, model_name=model_name, csv_filename='./evaluation_result/L3_nonindependent.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('./evaluation_result/L3_nonindependent.csv')\n",
    "\n",
    "evalu = 'ROC AUC'\n",
    "def filter_rows(df):\n",
    "    sgkf_row = df[df['Method'] == 'sgkf']\n",
    "    skf_row = df[df['Method'] == 'skf']\n",
    "    if not sgkf_row.empty and not skf_row.empty:\n",
    "        if skf_row[evalu].values[0] > sgkf_row[evalu].values[0]:\n",
    "            return pd.concat([sgkf_row, skf_row])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Apply the function to each group\n",
    "filtered_df = results_df.groupby('Model').apply(filter_rows).reset_index(drop=True)\n",
    "filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('./evaluation_result/model_evaluation_results.csv')\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5c4b974306676ed3503f20e11dce680c9511de066bf61ea7cafc97474d434a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
